# web_ui.py
import gradio as gr
import logging
import time
from model_manager import ModelManager
from gpu_utils import get_gpu_info
import os
logger = logging.getLogger(__name__)
model_manager_instance: ModelManager = None
UI_REFRESH_INTERVAL = 2
LOG_REFRESH_INTERVAL = 1
def get_gpu_status():
    try:
        gpus = get_gpu_info()
        if not gpus: return [["N/A", "æœªæ£€æµ‹åˆ°NVIDIA GPU", "N/A", "N/A"]]
        return [
            [gpu.id, gpu.name, f"{gpu.memoryUsed:.0f} / {gpu.memoryTotal:.0f} MB", f"{gpu.load * 100:.1f}%"]
            for gpu in gpus
        ]
    except Exception as e:
        logger.error(f"WebUIè·å–GPUä¿¡æ¯å¤±è´¥: {e}")
        return []
def refresh_ui_data():
    while True:
        try:
            gpu_status = get_gpu_status()
            all_model_status = model_manager_instance.get_all_models_status()
            
            active_models = [name for name, data in all_model_status.items() if data['status'] != 'stopped']
            log_dropdown_update = gr.update(choices=active_models if active_models else ["æ— æ´»åŠ¨æ¨¡å‹"])
            
            status_updates = []
            requests_updates = []
            for name, data in sorted(all_model_status.items()):
                status_updates.append(data['status'].capitalize())
                requests_updates.append(data['pending_requests'])
            
            yield (gpu_status, log_dropdown_update,
                   *status_updates, *requests_updates)
            
        except Exception as e:
            logger.error(f"UIåˆ·æ–°å¾ªç¯å‡ºé”™: {e}")
            yield (get_gpu_status() or []), gr.update(choices=["åˆ·æ–°å‡ºé”™"]), *([""] * 100)
        time.sleep(UI_REFRESH_INTERVAL)

def get_current_log_model_and_output(log_model_select_value, active_models):
    """è·å–å½“å‰åº”è¯¥æ˜¾ç¤ºçš„æ—¥å¿—æ¨¡å‹å’Œå¯¹åº”çš„æ—¥å¿—è¾“å‡º"""
    if not active_models:
        return "æ— æ´»åŠ¨æ¨¡å‹", "è¯·ä»ä¸Šæ–¹é€‰æ‹©ä¸€ä¸ªæ¨¡å‹ä»¥æŸ¥çœ‹å…¶æ—¥å¿—ã€‚"
    
    # å¦‚æœå½“å‰é€‰æ‹©çš„æ¨¡å‹ä¸åœ¨æ´»åŠ¨æ¨¡å‹åˆ—è¡¨ä¸­ï¼Œåˆ™é€‰æ‹©ç¬¬ä¸€ä¸ªæ´»åŠ¨æ¨¡å‹
    if log_model_select_value not in active_models and log_model_select_value != "æ— æ´»åŠ¨æ¨¡å‹":
        selected_model = active_models[0] if active_models else "æ— æ´»åŠ¨æ¨¡å‹"
    else:
        selected_model = log_model_select_value
    
    # è·å–é€‰ä¸­æ¨¡å‹çš„æ—¥å¿—
    if selected_model == "æ— æ´»åŠ¨æ¨¡å‹":
        log_output = "è¯·ä»ä¸Šæ–¹é€‰æ‹©ä¸€ä¸ªæ¨¡å‹ä»¥æŸ¥çœ‹å…¶æ—¥å¿—ã€‚"
    else:
        try:
            log_lines = model_manager_instance.get_model_log(selected_model)
            log_output = "\n".join(log_lines)
        except Exception as e:
            logger.error(f"è·å–æ—¥å¿—æ—¶å‡ºé”™ {selected_model}: {e}")
            log_output = f"è·å– {selected_model} æ—¥å¿—æ—¶å‡ºé”™ã€‚"
    
    return selected_model, log_output
def stream_log_output(model_name: str):
    if not model_name or model_name == "æ— æ´»åŠ¨æ¨¡å‹":
        yield "è¯·ä»ä¸Šæ–¹é€‰æ‹©ä¸€ä¸ªæ¨¡å‹ä»¥æŸ¥çœ‹å…¶æ—¥å¿—ã€‚"
        return
    while True:
        try:
            log_lines = model_manager_instance.get_model_log(model_name)
            yield "\n".join(log_lines)
        except Exception as e:
            logger.error(f"åˆ·æ–°æ—¥å¿—æ—¶å‡ºé”™ {model_name}: {e}")
            yield f"è·å– {model_name} æ—¥å¿—æ—¶å‡ºé”™ã€‚"
        
        time.sleep(LOG_REFRESH_INTERVAL)
def create_model_control_row(primary_name: str, status_data: dict, index: int):
    with gr.Row(variant="panel"):
        with gr.Column(scale=3):
            gr.Markdown(f"**{primary_name}**")
            mode_display = status_data.get('mode', 'Chat')
            mode_emoji = {"Chat": "ğŸ’¬", "Base": "ğŸ“", "Embedding": "ğŸ”"}.get(mode_display, "ğŸ¤–")
            gr.Markdown(f"<small>{mode_emoji} æ¨¡å¼: {mode_display}</small>")
            if status_data['aliases'][1:]:
                aliases_str = ", ".join(status_data['aliases'][1:])
                gr.Markdown(f"<small>åˆ«å: {aliases_str}</small>")
        
        # ä¿®æ”¹ç‚¹1ï¼šå°†çŠ¶æ€å’Œè¯·æ±‚æ”¾åœ¨åŒä¸€è¡Œçš„ä¸¤ä¸ªæ¡†ä¸­
        with gr.Column(scale=5, min_width=400):  # å¢åŠ å®½åº¦
            with gr.Row():
                status_box = gr.Textbox(
                    value=status_data['status'].capitalize(),
                    label="çŠ¶æ€",
                    interactive=False,
                    scale=1,
                    elem_id=f"status_box_{index}",
                    min_width=200  # å¢åŠ æœ€å°å®½åº¦
                )
                requests_box = gr.Textbox(
                    value=status_data['pending_requests'],
                    label="å¾…å¤„ç†è¯·æ±‚",
                    interactive=False,
                    scale=1,
                    elem_id=f"requests_box_{index}",
                    min_width=200  # å¢åŠ æœ€å°å®½åº¦
                )
        
        with gr.Column(scale=3, min_width=240):
            with gr.Row():
                start_btn = gr.Button("å¯åŠ¨", variant="primary", size="sm")
                stop_btn = gr.Button("åœæ­¢", variant="stop", size="sm")
    
    start_btn.click(
        fn=lambda p_name=primary_name: model_manager_instance.start_model(
            alias=p_name, 
            bypass_vram_check=model_manager_instance.get_model_config(p_name).get("bypass_vram_check", False)
        ),
        inputs=[], 
        outputs=[]
    ).then(lambda: gr.Info(f"å·²å‘é€å¯åŠ¨ '{primary_name}' çš„æŒ‡ä»¤..."), None, None)
    stop_btn.click(
        fn=lambda p_name=primary_name: model_manager_instance.stop_model(alias=p_name),
        inputs=None, 
        outputs=None
    ).then(lambda: gr.Info(f"å·²å‘é€åœæ­¢ '{primary_name}' çš„æŒ‡ä»¤..."), None, None)
    
    return status_box, requests_box
def run_web_ui(manager: ModelManager, host: str, port: int):
    global model_manager_instance
    model_manager_instance = manager
    
    custom_css = """
        .console-output textarea {
            background-color: #000000 !important;
            color: #ffffff !important;
            font-family: 'Consolas', 'Monaco', 'Courier New', monospace !important;
            font-size: 14px !important;
            line-height: 1.4 !important;
            border: 1px solid #333333 !important;
            border-radius: 4px !important;
            resize: none !important;
            height: 580px !important;
            max-height: 580px !important;
            overflow-y: auto !important;
            width: 100% !important;
            min-width: 100% !important;
        }
        
        /* å›ºå®šæ§åˆ¶å°è¾“å‡ºåŒºåŸŸé«˜åº¦å¹¶å¯ç”¨æ»šåŠ¨ */
        .console-output .gr-textbox {
            height: 600px !important;
            max-height: 600px !important;
            overflow-y: auto !important;
            flex-grow: 0 !important;
            flex-shrink: 0 !important;
            width: 100% !important;
            min-width: 100% !important;
        }
        
        /* æ§åˆ¶å°è¾“å‡ºåŒºåŸŸæ»šåŠ¨æ¡æ ·å¼ */
        .console-output .gr-textbox::-webkit-scrollbar {
            width: 12px !important;
        }
        
        .console-output .gr-textbox::-webkit-scrollbar-thumb {
            background: #888 !important;
            border-radius: 6px !important;
        }
        
        .console-output .gr-textbox::-webkit-scrollbar-thumb:hover {
            background: #555 !important;
        }
        
        /* å¼ºåˆ¶é™åˆ¶æ§åˆ¶å°è¾“å‡ºåŒºåŸŸå®¹å™¨é«˜åº¦ */
        .console-output {
            height: 600px !important;
            max-height: 600px !important;
            overflow-y: hidden !important;  /* æ”¹ä¸ºhiddenï¼Œé¿å…åŒå±‚æ»šåŠ¨æ¡ */
            flex-grow: 0 !important;
            flex-shrink: 0 !important;
            width: 100% !important;
            min-width: 100% !important;
        }
        
        /* ç¡®ä¿æ§åˆ¶å°è¾“å‡ºåŒºåŸŸçš„çˆ¶å®¹å™¨ä¸ä¼šæ‰©å±• */
        .console-output-wrapper {
            height: 600px !important;
            max-height: 600px !important;
            overflow: hidden !important;
            flex-grow: 0 !important;
            flex-shrink: 0 !important;
            width: 100% !important;
            min-width: 100% !important;
        }
        
        .model-scroll-container {
            height: 600px !important;  /* å‡å°‘æ¨¡å‹é¢æ¿é«˜åº¦ */
            overflow-y: auto !important;
            padding-right: 10px !important;
            border: 1px solid #e0e0e0 !important;
            border-radius: 8px !important;
            background-color: #f8f9fa !important;
        }
        
        .model-scroll-container::-webkit-scrollbar {
            width: 12px !important;
        }
        
        .model-scroll-container::-webkit-scrollbar-thumb {
            background: #888 !important;
            border-radius: 6px !important;
        }
        
        .model-scroll-container::-webkit-scrollbar-thumb:hover {
            background: #555 !important;
        }
        
        #main-title {
            text-align: center;
            margin-bottom: 20px !important;
        }
        
        .gpu-monitor {
            margin-bottom: 20px;
            padding: 15px;
        }
        
        .model-panel {
            padding: 15px;
            height: 100%;
            display: flex;
            flex-direction: column;
        }
        
        .log-panel {
            padding: 15px;
            height: 100%;
            display: flex;
            flex-direction: column;
        }
        
        /* ç¡®ä¿æ¨¡å‹é¢æ¿å’Œæ—¥å¿—é¢æ¿é«˜åº¦ä¸€è‡´ */
        .panel-content {
            flex: 1;
            display: flex;
            flex-direction: column;
        }
        
        /* ç§»é™¤ä¹‹å‰çš„é«˜åº¦é™åˆ¶ï¼Œä½¿ç”¨æ–°çš„æ ·å¼ */
        .log-panel .gr-box {
            height: auto !important;
        }
        
        /* å¼ºåˆ¶é™åˆ¶æ—¥å¿—é¢æ¿ä¸­çš„æ‰€æœ‰å…ƒç´ é«˜åº¦ */
        .log-panel .gradio-row {
            flex-grow: 0 !important;
            flex-shrink: 0 !important;
            width: 100% !important;
        }
        
        /* ç¡®ä¿æ—¥å¿—é¢æ¿ä¸­çš„æœ€åä¸€è¡Œï¼ˆåŒ…å«æ§åˆ¶å°è¾“å‡ºï¼‰ä¸ä¼šæ‰©å±• */
        .log-panel .panel-content > :last-child {
            flex-grow: 0 !important;
            flex-shrink: 0 !important;
            width: 100% !important;
        }
        
        /* å¼ºåˆ¶é™åˆ¶æ•´ä¸ªæ—¥å¿—é¢æ¿çš„é«˜åº¦ */
        .log-panel {
            max-height: 800px !important;
            overflow: hidden !important;
            width: 100% !important;
        }
        
        /* ç¡®ä¿æ§åˆ¶å°è¾“å‡ºåŒºåŸŸå®½åº¦æ­£å¸¸ */
        .console-output-wrapper .gradio-row {
            width: 100% !important;
            min-width: 100% !important;
        }
        
        /* ç¡®ä¿æ§åˆ¶å°è¾“å‡ºåŒºåŸŸçš„çˆ¶å®¹å™¨å®½åº¦æ­£å¸¸ */
        .console-output-wrapper {
            width: 100% !important;
            min-width: 100% !important;
        }
        
        .gradio-container {
            background-color: inherit !important;
        }
    """
    
    with gr.Blocks(title="LLM-Manager", theme=gr.themes.Soft(), css=custom_css) as ui:
        gr.Markdown("# ğŸ§  LLM-Manager æ§åˆ¶å°", elem_id="main-title")
        
        status_boxes = []
        requests_boxes = []
        
        with gr.Row():
            with gr.Column(elem_classes="gpu-monitor"):
                gr.Markdown("## ğŸ–¥ï¸ GPU å®æ—¶ç›‘æ§")
                gpu_table = gr.DataFrame(
                    headers=["ID", "åç§°", "æ˜¾å­˜ä½¿ç”¨", "å ç”¨ç‡"],
                    datatype=["str", "str", "str", "str"],
                    interactive=False,
                    row_count=(len(get_gpu_info()) or 1, "fixed")
                )
        
        with gr.Row():
            # ä¿®æ”¹ç‚¹3ï¼šè°ƒæ•´åˆ—æ¯”ä¾‹ï¼Œæ¨¡å‹é¢æ¿å æ¯”æ›´å¤§
            with gr.Column(scale=1, elem_classes="model-panel"):
                gr.Markdown("## ğŸš€ æ¨¡å‹æ§åˆ¶é¢æ¿")
                gr.Markdown("<small>é¢æ¿ä¸­çš„æ¨¡å‹çŠ¶æ€å’Œè¯·æ±‚æ•°ä¼šå®æ—¶æ›´æ–°ã€‚</small>")
                
                with gr.Column(elem_classes="panel-content"):
                    with gr.Column(elem_classes="model-scroll-container"):
                        all_model_status = manager.get_all_models_status()
                        if not all_model_status:
                            gr.Markdown("âŒ **é”™è¯¯**: `config.json` ä¸­æ²¡æœ‰æ‰¾åˆ°ä»»ä½•æ¨¡å‹é…ç½®ã€‚")
                        else:
                            sorted_models = sorted(all_model_status.items())
                            for i, (name, data) in enumerate(sorted_models):
                                status_box, requests_box = create_model_control_row(name, data, i)
                                status_boxes.append(status_box)
                                requests_boxes.append(requests_box)
            
            # ä¿®æ”¹ç‚¹4ï¼šæ—¥å¿—é¢æ¿å æ¯”æ›´å°
            with gr.Column(scale=3, elem_classes="log-panel"):
                gr.Markdown("## ğŸ“œ æ¨¡å‹å®æ—¶æ—¥å¿—")
                gr.Markdown("<small>âš ï¸ æ³¨æ„ï¼šå½“å¯åŠ¨æˆ–åœæ­¢æ¨¡å‹åï¼Œè¯·æ‰‹åŠ¨åˆ·æ–°é¡µé¢ä»¥æ›´æ–°æ—¥å¿—æ˜¾ç¤ºé€‰é¡¹ã€‚</small>")
                with gr.Column(elem_classes="panel-content"):
                    with gr.Row():
                        log_model_select = gr.Dropdown(
                            label="é€‰æ‹©ä¸€ä¸ªæ´»åŠ¨ä¸­çš„æ¨¡å‹æŸ¥çœ‹æ—¥å¿—",
                            choices=["æ— æ´»åŠ¨æ¨¡å‹"],
                            container=False
                        )
                    with gr.Row(elem_classes="console-output-wrapper"):
                        # ä¿®æ”¹ç‚¹5ï¼šå›ºå®šæ§åˆ¶å°é«˜åº¦å¹¶å¯ç”¨æ»šåŠ¨
                        log_output = gr.Textbox(
                            label="æ§åˆ¶å°è¾“å‡º",
                            lines=30,  # å¢åŠ è¡Œæ•°ä»¥é€‚åº”å›ºå®šé«˜åº¦
                            interactive=False,
                            max_lines=2000,
                            autoscroll=False,  # ç¦ç”¨è‡ªåŠ¨æ»šåŠ¨ï¼Œè®©ç”¨æˆ·æ§åˆ¶æ»šåŠ¨ä½ç½®
                            elem_classes="console-output"
                        )
        
        ui.load(
            fn=refresh_ui_data,
            inputs=None,
            outputs=[gpu_table, log_model_select] + status_boxes + requests_boxes
        )
        
        # æ·»åŠ ä¸€ä¸ªå‡½æ•°æ¥å¤„ç†æ¨¡å‹åˆ‡æ¢æ—¶çš„æç¤º
        def show_refresh_hint():
            """æ˜¾ç¤ºåˆ·æ–°æç¤º"""
            return "âš ï¸ æ¨¡å‹çŠ¶æ€å·²å˜æ›´ï¼Œè¯·æ‰‹åŠ¨åˆ·æ–°é¡µé¢ä»¥æ›´æ–°æ—¥å¿—æ˜¾ç¤ºé€‰é¡¹ã€‚"
        
        # ä¿®æ”¹UIåˆ·æ–°é€»è¾‘ï¼Œå½“æ´»åŠ¨æ¨¡å‹å˜æ›´æ—¶æ˜¾ç¤ºæç¤º
        ui.load(
            fn=refresh_ui_data,
            inputs=None,
            outputs=[gpu_table, log_model_select] + status_boxes + requests_boxes
        )
        
        log_model_select.change(
            fn=stream_log_output,
            inputs=log_model_select,
            outputs=log_output,
            show_progress="hidden"
        )
        
    logger.info(f"WebUI ç®¡ç†ç•Œé¢å°†åœ¨ http://{host}:{port} ä¸Šå¯åŠ¨")
    # ä¿®æ”¹ç‚¹5ï¼šç¡®ä¿æ§åˆ¶å°ä½¿ç”¨chcp 65001é€‚é…ä¸­æ–‡
    os.environ["GRADIO_SERVER_NAME"] = host
    os.environ["GRADIO_SERVER_PORT"] = str(port)
    # åœ¨å¯åŠ¨å‰è®¾ç½®æ§åˆ¶å°ç¼–ç 
    os.system("chcp 65001 >nul")
    ui.queue().launch(server_name=host, server_port=port, share=False)