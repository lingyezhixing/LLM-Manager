# LLM-Manager

**LLM-Manager** 是一个用于统一管理本地大型语言模型（LLM）的后端服务与监控工具。它通过统一 API 接口和动态资源调度，简化多模型在本地环境中的部署与调用流程。

> **⚠️ 重要说明**：  
> 本项目为个人开发工具，适用于本地实验环境。  
> 不包含任何模型文件。用户需自行准备模型启动脚本（如 `.bat` 或 `.sh`）。  
> 使用前需具备 Python 和本地 LLM 部署的基础能力。

---

## 核心功能

1. **统一 API 接口**  
   提供兼容 OpenAI 格式的标准接口：  
   - `/v1/chat/completions`  
   - `/v1/embeddings`  
   - `/v1/rerank`（Reranker 模式）  
   请求自动路由至对应本地模型服务端口。

2. **插件化架构**  
   - **接口插件**：支持 `Chat`、`Base`、`Embedding`、`Reranker` 四种模型模式。  
   - **设备插件**：检测 NVIDIA GPU（RTX 系列 / 数据中心卡）状态，用于动态调度。

3. **智能资源调度**  
   - **按需启动**：请求到达时自动启动模型，空闲超时后关闭以释放显存。  
   - **环境适配**：根据当前在线显卡型号（如 4060、V100）自动选择匹配的启动参数。  
   - **并发控制**：优化高并发下的冷启动流程，避免线程阻塞。

4. **数据监控与计费**  
   - 使用 SQLite 记录请求日志。  
   - 支持两种计费模式：  
     - 按 Token 消耗（阶梯定价）  
     - 按使用时长（租赁场景）  
   - 提供 WebUI 实时展示吞吐量、成本统计与日志流。

5. **跨平台支持**  
   支持 Windows；Linux 适配处于开发中，路径与脚本处理已做兼容性设计。

---

## 安装与启动

### 1. 环境要求
- Python 3.10+
- SQLite3（通常随系统或 Python 自动安装）
- Node.js 18+（仅用于前端构建；项目已包含预构建的 WebUI 文件）

### 2. 后端设置
```bash
# 克隆仓库
git clone https://github.com/lingyezhixing/LLM-Manager.git
cd LLM-Manager

# 安装依赖
pip install -r requirements.txt
```

> **配置文件**：  
> 项目不提供 `config.yaml` 示例。用户需**自行创建** `config.yaml` 文件于项目根目录，结构参考下文。

### 3. 启动服务
```bash
python main.py
```

启动后访问：`http://localhost:8080`

---

## 配置文件 (`config.yaml`)

请在项目根目录手动创建 `config.yaml` 文件。该文件为 YAML 格式，包含程序基础配置与模型定义。

### 程序基础配置
```yaml
program:
  host: "0.0.0.0"
  port: 8080
  log_level: "INFO"
  alive_time: 15          # 模型空闲超时时间（分钟），超时后自动关闭
  Disable_GPU_monitoring: false # 是否禁用 GPU 资源监控。禁用后，即使资源不足仍尝试启动模型。
```

### 模型配置 (`Local-Models`)

每个模型需定义唯一标识、运行模式、端口及启动脚本。支持双级配置：优先使用 `Dual-GPU-Config`，设备不满足时回退至 `Single-GPU-Config`。

```yaml
Local-Models:
  Qwen-14B-Chat:
    aliases: ["gpt-3.5-turbo", "qwen-14b"]  # API 调用时使用的模型名称映射
    mode: "Chat"                            # 模式：Chat / Base / Embedding / Reranker
    port: 10001                             # 模型服务监听端口
    auto_start: false                       # 是否随服务启动

    Dual-GPU-Config:
      required_devices: ["rtx 4060", "v100"] # 必须同时在线的设备
      script_path: "scripts/qwen_dual.bat"   # 启动脚本路径（Windows）或 .sh（Linux）
      memory_mb:
        "rtx 4060": 8000
        "v100": 16000

    Single-GPU-Config:
      required_devices: ["v100"]
      script_path: "scripts/qwen_single.bat"
      memory_mb:
        "v100": 24000
```

> ✅ **说明**：  
> - `script_path` 需指向用户自行编写的启动脚本，确保其可执行并正确绑定指定端口。  
> - `required_devices` 中的设备名称需与系统识别名称一致（如通过 `nvidia-smi` 查看）。  
> - 若未配置 `Dual-GPU-Config`，仅使用 `Single-GPU-Config` 即可。

---

## 更新日志 (Changelog)

### v2.2.0 - 2026-01-04 (Current)
**性能优化与资源管理**
- **按需监控机制**：实现智能监控线程管理，10秒无API请求时自动停止监控，降低设备功耗和驱动开销
- **前后端同步优化**：设备状态刷新间隔统一优化为1秒，减少不必要的轮询请求
- **智能缓存刷新**：模型启动/停止后自动刷新设备状态，确保硬件资源信息实时准确
- **代码重构**：统一设备状态更新接口，移除重复函数；优化CPU监控避免阻塞调用
- **托盘功能增强**：系统托盘新增手动刷新设备状态功能，方便随时查看硬件状态
- **监控精度提升**：前端轮询与后端刷新频率对齐，提供更流畅的实时监控体验

### v2.1.2 - 2025-12-18
**稳定性修复**
- 修复了空闲检查时间不更新导致的意外关闭问题。

### v2.1.1 - 2025-12-03
**稳定性修复**
- 修复高并发请求触发模型冷启动时导致的线程池耗尽（Thread Pool Starvation）问题。
- 在 Router 层引入本地异步锁，确保同一模型启动过程仅单线程执行，其余请求等待。
- 优化配置文件加载逻辑，避免重复解析。

### v2.1.0 - 2025-11-23
**架构与配置升级**
- 配置文件格式从 `.json` 迁移至 `.yaml`，提升可读性与扩展性。
- 初步支持 Linux 环境，统一路径处理逻辑（使用 `script_path` 替代平台相关字段）。
- 修复进程管理器在无运行进程时的超时异常，提升关闭效率。
- 重构日志模块，解决多实例初始化导致的日志丢失问题。

### v2.0.0 - 2025-10-26
**WebUI 重构与计费系统**
- 前端界面完全重写，集成实时监控面板。
- 新增按时间计费模式（适用于算力租赁场景）。
- 完善 Token 阶梯计费逻辑，支持账单数据管理。
- 后端统计计算全面向量化，显著降低接口延迟。

### v1.1.0 - 2025-10-07
**并发与死锁修复**
- 修复模型启动/停止时的死锁问题，引入带超时的锁机制。
- 支持中断正在启动中的模型进程。
- 优化 `auto_start` 逻辑，避免重复启动。

### v1.0.0 - 2025-09-22
**架构重构 (里程碑)**
- 实现插件化设计：接口插件与设备插件动态加载。
- 模块化拆分：分离 `ProcessManager`、`ConfigManager`、`Monitor`。
- 所有 I/O 操作异步化，避免阻塞 API 线程。
- 新增功能：
  - Token 消耗追踪
  - 模型运行时间监控
  - 系统托盘服务支持（WebUI 快捷入口）

### v0.x.x - 早期版本 (2025-09)
- v0.5.0: 新增 Reranker 模式 (`/v1/rerank`)  
- v0.4.0: 实现动态 GPU 优先级配置系统，支持多环境适配  
- v0.3.0: 新增 Embedding 模式支持  
- v0.1.0: 项目初始化，基础进程管理与 API 转发功能
```